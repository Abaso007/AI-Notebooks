{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Alternative PygmalionAI notebook for Google Colab\n",
        "\n",
        "This notebook is an *unofficial* method to run the [PygmalionAI LLMs](https://huggingface.co/PygmalionAI) (NSFW) using **oobabooga**'s [text-generation-webui](https://github.com/oobabooga/text-generation-webui) project in chat mode.\n",
        "\n",
        "Run all the cells and a public gradio URL will appear at the bottom in around 12 minutes.\n",
        "\n",
        "You can create your own custom characters using the [Pygmalion JSON character creator](https://oobabooga.github.io/character-creator.html)\n",
        "\n",
        "Original author: https://github.com/81300"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "# From: https://github.com/henk717/KoboldAI\n",
        "\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive (Uses only 13MB of data)</b><br/>\n",
        "<audio src=\"https://henk.tech/colabkobold/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Select the model and start the web UI\n",
        "\n",
        "%cd /tmp\n",
        "!wget -c https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "%env PYTHONPATH=\n",
        "!which conda 2>/dev/null || bash Miniconda3-latest-Linux-x86_64.sh -b -p /usr/local -f\n",
        "%env CI=true\n",
        "!conda init -q bash\n",
        "!source /usr/local/bin/activate && conda update -y -q -c defaults --all\n",
        "!source /usr/local/bin/activate && conda create -y -q -n textgen\n",
        "!source /usr/local/bin/activate && conda activate textgen && conda install -y -q -c defaults conda python\n",
        "!source /usr/local/bin/activate && conda activate textgen && conda install -y -q -c pytorch -c nvidia pytorch torchvision torchaudio pytorch-cuda=11.7\n",
        "!source /usr/local/bin/activate && conda clean -y -q --all\n",
        "\n",
        "# Download Github repo\n",
        "%cd /content\n",
        "!git clone https://github.com/oobabooga/text-generation-webui\n",
        "!ln -s text-generation-webui/models .\n",
        "!ln -s text-generation-webui/characters .\n",
        "!ln -s text-generation-webui/presets .\n",
        "%cd text-generation-webui\n",
        "!wget https://oobabooga.github.io/settings-colab.json\n",
        "\n",
        "# Install requirements\n",
        "!source /usr/local/bin/activate && conda activate textgen \\\n",
        "  && pip install --no-cache-dir --progress-bar=off --root-user-action=ignore -r requirements.txt diffusers\n",
        "\n",
        "Model = \"Pygmalion 6B-original (yields better results)\" #@param [\"Pygmalion 6B-original (yields better results)\", \"Pygmalion 6B\", \"Pygmalion 6B experimental\", \"Pygmalion 2.7B\", \"Pygmalion 1.3B\", \"Pygmalion 350M\"] {allow-input: false}\n",
        "\n",
        "if Model == \"Pygmalion 6B-original (yields better results)\":\n",
        "  huggingface_org = \"PygmalionAI\"\n",
        "  huggingface_repo = \"pygmalion-6b\"\n",
        "  huggingface_branch = \"b8344bb4eb76a437797ad3b19420a13922aaabe1\"\n",
        "  model_name = \"pygmalion-6b_b8344bb4eb76a437797ad3b19420a13922aaabe1\"\n",
        "elif Model == \"Pygmalion 6B\":\n",
        "  huggingface_org = \"PygmalionAI\"\n",
        "  huggingface_repo = \"pygmalion-6b\"\n",
        "  huggingface_branch = \"main\"\n",
        "  model_name = \"pygmalion-6b\"\n",
        "elif Model == \"Pygmalion 6B experimental\":\n",
        "  huggingface_org = \"PygmalionAI\"\n",
        "  huggingface_repo = \"pygmalion-6b\"\n",
        "  huggingface_branch = \"dev\"\n",
        "  model_name = \"pygmalion-6b_dev\"\n",
        "elif Model == \"Pygmalion 2.7B\":\n",
        "  huggingface_org = \"PygmalionAI\"\n",
        "  huggingface_repo = \"pygmalion-2.7b\"\n",
        "  huggingface_branch = \"main\"\n",
        "  model_name = \"pygmalion-2.7b\"\n",
        "elif Model == \"Pygmalion 1.3B\":\n",
        "  huggingface_org = \"PygmalionAI\"\n",
        "  huggingface_repo = \"pygmalion-1.3b\"\n",
        "  huggingface_branch = \"main\"\n",
        "  model_name = \"pygmalion-1.3b\"\n",
        "elif Model == \"Pygmalion 350M\":\n",
        "  huggingface_org = \"PygmalionAI\"\n",
        "  huggingface_repo = \"pygmalion-350m\"\n",
        "  huggingface_branch = \"main\"\n",
        "  model_name = \"pygmalion-350m\"\n",
        "\n",
        "# Download the model\n",
        "![[ ! -f models/$model_name/config.json ]] && source /usr/local/bin/activate && conda activate textgen \\\n",
        "  && python download-model.py $huggingface_org/$huggingface_repo --branch $huggingface_branch\n",
        "\n",
        "!source /usr/local/bin/activate && conda activate textgen \\\n",
        "  && python server.py --share --cai-chat --no-stream --auto-devices --model $model_name --settings settings-colab.json"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

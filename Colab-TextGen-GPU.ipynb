{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Alternative PygmalionAI notebook for Google Colab\n",
        "\n",
        "This notebook is an *unofficial* method to run [PygmalionAI LLMs](https://huggingface.co/PygmalionAI) (NSFW) that uses **oobabooga**'s [text-generation-webui project](https://github.com/oobabooga/text-generation-webui) in chat mode.\n",
        "\n",
        "Run all the cells and a public gradio URL will appear at the bottom in around 12 minutes.\n",
        "\n",
        "Author: https://github.com/81300"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "# From: https://github.com/henk717/KoboldAI\n",
        "\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive (Uses only 13MB of data)</b><br/>\n",
        "<audio src=\"https://henk.tech/colabkobold/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Select the model and start the web UI\n",
        "\n",
        "%cd /tmp\n",
        "!wget -c https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "%env PYTHONPATH=\n",
        "!which conda 2>/dev/null || bash Miniconda3-latest-Linux-x86_64.sh -b -p /usr/local -f\n",
        "%env CI=true\n",
        "!conda init -q bash\n",
        "!source /usr/local/bin/activate && conda update -y -q -c defaults --all\n",
        "!source /usr/local/bin/activate && conda create -y -q -n textgen\n",
        "!source /usr/local/bin/activate && conda activate textgen && conda install -y -q -c defaults conda python\n",
        "!source /usr/local/bin/activate && conda activate textgen && conda install -y -q -c pytorch -c nvidia pytorch torchvision torchaudio pytorch-cuda=11.7\n",
        "!source /usr/local/bin/activate && conda clean -y -q --all\n",
        "\n",
        "import os\n",
        "if not os.path.exists(\"/content/drive\"):\n",
        "  os.mkdir(\"/content/drive\")\n",
        "if not os.path.exists(\"/content/drive/MyDrive/\"):\n",
        "  os.mkdir(\"/content/drive/MyDrive/\")\n",
        "\n",
        "# Download Github repo\n",
        "!git -C /content/drive/MyDrive/Colab-TextGen pull || git clone https://github.com/oobabooga/text-generation-webui /content/drive/MyDrive/Colab-TextGen\n",
        "\n",
        "%cd /content/drive/MyDrive/Colab-TextGen\n",
        "\n",
        "# Set model directories outside of Google Drive\n",
        "!mountpoint -q models || mv -f models models_backup\n",
        "!mountpoint -q torch-dumps || mv -f torch-dumps torch-dumps_backup\n",
        "!mkdir -p models torch-dumps /content/models /content/torch-dumps\n",
        "!mountpoint -q models || mount --bind /content/models models\n",
        "!mountpoint -q torch-dumps || mount --bind /content/torch-dumps torch-dumps\n",
        "\n",
        "# Install requirements\n",
        "!source /usr/local/bin/activate && conda activate textgen \\\n",
        "  && pip install --no-cache-dir --progress-bar=off --root-user-action=ignore -r requirements.txt diffusers\n",
        "\n",
        "Model = \"Pygmalion 6B\" #@param [\"Pygmalion 6B\", \"Pygmalion 6B experimental\", \"Pygmalion 2.7B\", \"Pygmalion 1.3B\", \"Pygmalion 350M\"] {allow-input: false}\n",
        "\n",
        "if Model == \"Pygmalion 6B\":\n",
        "  huggingface_org = \"PygmalionAI\"\n",
        "  huggingface_repo = \"pygmalion-6b\"\n",
        "  huggingface_branch = \"main\"\n",
        "  model_name = \"pygmalion-6b\"\n",
        "elif Model == \"Pygmalion 6B experimental\":\n",
        "  huggingface_org = \"PygmalionAI\"\n",
        "  huggingface_repo = \"pygmalion-6b\"\n",
        "  huggingface_branch = \"dev\"\n",
        "  model_name = \"pygmalion-6b_dev\"\n",
        "elif Model == \"Pygmalion 2.7B\":\n",
        "  huggingface_org = \"PygmalionAI\"\n",
        "  huggingface_repo = \"pygmalion-2.7b\"\n",
        "  huggingface_branch = \"main\"\n",
        "  model_name = \"pygmalion-2.7b\"\n",
        "elif Model == \"Pygmalion 1.3B\":\n",
        "  huggingface_org = \"PygmalionAI\"\n",
        "  huggingface_repo = \"pygmalion-1.3b\"\n",
        "  huggingface_branch = \"main\"\n",
        "  model_name = \"pygmalion-1.3b\"\n",
        "elif Model == \"Pygmalion 350M\":\n",
        "  huggingface_org = \"PygmalionAI\"\n",
        "  huggingface_repo = \"pygmalion-350m\"\n",
        "  huggingface_branch = \"main\"\n",
        "  model_name = \"pygmalion-350m\"\n",
        "\n",
        "%cd /content/drive/MyDrive/Colab-TextGen\n",
        "\n",
        "# Download the model\n",
        "![[ ! -f models/$model_name/config.json ]] && source /usr/local/bin/activate && conda activate textgen \\\n",
        "  && python download-model.py $huggingface_org/$huggingface_repo --branch $huggingface_branch\n",
        "\n",
        "launch_model = model_name\n",
        "\n",
        "%cd /content/drive/MyDrive/Colab-TextGen\n",
        "\n",
        "!source /usr/local/bin/activate && conda activate textgen \\\n",
        "  && python server.py --share --cai-chat --no-stream --auto-devices --model $launch_model"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
